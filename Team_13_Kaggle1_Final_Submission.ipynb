{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tharungajula2/Portfolio/blob/main/Team_13_Kaggle1_Final_Submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Model**\n"
      ],
      "metadata": {
        "id": "wqk2Z-F405ku"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "\n",
        "### Mini Project Notebook: Multi-Text Classification of Coronavirus Tweets using Deep Neural Networks (RNNs).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maritime-miami"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nljJR6CwfZN_"
      },
      "source": [
        "At the end of the mini-hackathon, you will be able to :\n",
        "\n",
        "* perform data preprocessing/preprocess the text\n",
        "* represent the text/words using the pretrained word embeddings - Word2Vec/Glove\n",
        "* build the deep neural network (RNN, LSTM, GRU, CNNs, Bidirectional-LSTM, GRU,BERT) to classify the tweets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "First we need to understand why sentiment analysis is needed for social media?\n",
        "\n",
        "People from all around the world have been using social media more than ever. Sentiment analysis on social media data helps to understand the wider public opinion about certain topics such as movies, events, politics, sports, and more and gain valuable insights from this social data. Sentiment analysis has some powerful applications. Nowadays it is also used by some businesses to do market research and understand the customer’s experiences for their products or services.\n",
        "\n",
        "Now an interesting question about this type of problem statement that may arise in your mind is that why sentiment analysis on COVID-19 Tweets? What is about the coronavirus tweets that would be positive? You may have heard sentiment analysis on movie or book reviews, but what is the purpose of exploring and analyzing this type of data?\n",
        "\n",
        "The use of social media for communication during the time of crisis has increased remarkably over the recent years. As mentioned above, analyzing social media data is important as it helps understand public sentiment. During the coronavirus pandemic, many people took to social media to express their anger, grief, or sadness while some also spread happiness and positivity. People also used social media to ask their network for help related to vaccines or hospitals during this hard time. Many issues related to this pandemic can also be solved if experts considered this social data. That’s the reason why analyzing this type of data is important to understand the overall issues faced by people.\n",
        "\n"
      ],
      "metadata": {
        "id": "iNI_-0spy1Ho"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL0Ve1abn6YJ"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The given challenge is to build a multiclass classification model to predict the sentiment of Covid-19 tweets. The tweets have been pulled from Twitter and manual tagging has been done. We are given information like Location, Tweet At, Original Tweet, and Sentiment.\n",
        "\n",
        "The training dataset consists of 36000 tweets and the testing dataset consists of 8955 tweets. There are 5 sentiments namely ‘Positive’, ‘Extremely Positive’, ‘Negative’, ‘Extremely Negative’, and ‘Neutral’ in the sentiment column.\n",
        "\n",
        "## Description\n",
        "\n",
        "This dataset has the following information about the user who tweeted:\n",
        "\n",
        "1. **UserName:** twitter handler\n",
        "2. **ScreenName:** a personal identifier on Twitter and is separate from the username\n",
        "3. **Location:** where in the world the person tweets from\n",
        "4. **TweetAt:** date of the tweet posted (DD-MM-YYYY)\n",
        "5. **OriginalTweet:** the tweet itself\n",
        "6. **Sentiment:** sentiment value\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWGmjNHdZul"
      },
      "source": [
        "To build and implement a multiclass classification deep neural network model to classify between Positive/Extremely Positive/Negative/Extremely Negative/Neutral sentiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BQEA97zTlTa"
      },
      "source": [
        "## Grading = 10 Marks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdYmy-tJgURN"
      },
      "source": [
        "Here is a handy link to Kaggle's competition documentation (https://www.kaggle.com/docs/competitions), which includes, among other things, instructions on submitting predictions (https://www.kaggle.com/docs/competitions#making-a-submission)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8OapRtHgLnU"
      },
      "source": [
        "## Instructions for downloading train and test dataset from Kaggle API are as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO2jS73oLnCR"
      },
      "source": [
        "### 1. Create an API key in Kaggle.\n",
        "\n",
        "To do this, go to the competition site on Kaggle at (https://www.kaggle.com/t/6ff3f8dbf34a4a57af7eac66dded4f31) and open your user settings page. Click Account.\n",
        "\n",
        "* Click on your profile picture at the top-right corner of the page.\n",
        "\n",
        "![alt text](https://i.imgur.com/kSLmEj2.png)\n",
        "\n",
        "* In the popout menu, click the Settings option.\n",
        "\n",
        "![alt text](https://i.imgur.com/tNi6yun.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkzGffHdbwX2"
      },
      "source": [
        "### 2. Next, scroll down to the API access section and click generate to download an API key (kaggle.json).\n",
        "![alt text](https://i.imgur.com/vRNBgrF.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtETuXx8b-OC"
      },
      "source": [
        "### 3. Upload your kaggle.json file using the following snippet in a code cell:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Start"
      ],
      "metadata": {
        "id": "Scfe74ROe-D9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ci6gdcjGfXmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1pfXBDxWl0Y"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCV_T6MMW4eX"
      },
      "source": [
        "#If successfully uploaded in the above step, the 'ls' command here should display the kaggle.json file.\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbukdzJ6cE32"
      },
      "source": [
        "### 4. Install the Kaggle API using the following command\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMj1n1MJcqzN"
      },
      "source": [
        "!pip install -U -q kaggle==1.5.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vpy9P1nchhd"
      },
      "source": [
        "### 5. Move the kaggle.json file into ~/.kaggle, which is where the API client expects your token to be located:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQbPsDOLZ0b4"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BenAWlpI73sm"
      },
      "source": [
        "# Execute the following command to verify whether the kaggle.json is stored in the appropriate location: ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm2jGsCradOS"
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json # run this command to ensure your Kaggle API token is secure on colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32unPZKzdI72"
      },
      "source": [
        "### 6. Now download the Test Data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppuy5gRKHFwv"
      },
      "source": [
        "**NOTE: If you get a '404 - Not Found' error after running the cell below, it is most likely that the user (whose kaggle.json is uploaded above) has not 'accepted' the rules of the competition and therefore has 'not joined' the competition.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41-ETZCE_A1j"
      },
      "source": [
        "If you encounter **401-unauthorised** download latest **kaggle.json** by repeating steps 1 & 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY40TmgfHq0s"
      },
      "source": [
        "#If you get a forbidden link, you have most likely not joined the competition.\n",
        "!kaggle competitions download -c multi-text-classification-of-coronavirus-tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/multi-text-classification-of-coronavirus-tweets.zip"
      ],
      "metadata": {
        "id": "mvKRiFNglvpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOUR CODING STARTS FROM HERE"
      ],
      "metadata": {
        "id": "QeKon2vruI_c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abstract-stocks"
      },
      "source": [
        "## Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "XFq7ri6wLyFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53g0zVbjRV7K"
      },
      "source": [
        "##   **Stage 1**:  Data Loading and Perform Exploratory Data Analysis (1 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Load the Dataset\n"
      ],
      "metadata": {
        "id": "xIa9LlhMHj5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "train_data = pd.read_csv('/content/corona_nlp_train.csv/corona_nlp_train.csv', encoding='ISO-8859-1')\n",
        "test_data = pd.read_csv('/content/corona_nlp_test.csv/corona_nlp_test.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# Display the first few rows of each dataframe\n",
        "print(\"Train Data\")\n",
        "print(train_data.head())\n",
        "print(\"\\nTest Data\")\n",
        "print(test_data.head())"
      ],
      "metadata": {
        "id": "6qmR5Vo_tbVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Check for Missing Values"
      ],
      "metadata": {
        "id": "hzQS91rfJLNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Handle missing values in 'Location' by filling with 'Unknown'\n",
        "train_data['Location'] = train_data['Location'].fillna('Unknown')\n",
        "test_data['Location'] = test_data['Location'].fillna('Unknown')"
      ],
      "metadata": {
        "id": "JF3xCD9qJN1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in the train data\n",
        "print(\"Missing values in train data:\")\n",
        "print(train_data.isnull().sum())\n",
        "\n",
        "# Check for missing values in the test data\n",
        "print(\"\\nMissing values in test data:\")\n",
        "print(test_data.isnull().sum())"
      ],
      "metadata": {
        "id": "vDyR6t1njJuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize the sentiment column values\n"
      ],
      "metadata": {
        "id": "nra2K6EPHosw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment mapping based on the actual values in the dataset\n",
        "sentiment_mapping = {\n",
        "    'Extremely Positive': 4,\n",
        "    'Positive': 3,\n",
        "    'Neutral': 2,\n",
        "    'Negative': 1,\n",
        "    'Extremely Negative': 0\n",
        "}\n",
        "\n",
        "# Map sentiment values to numeric values\n",
        "train_data['Sentiment'] = train_data['Sentiment'].map(sentiment_mapping)\n",
        "\n",
        "# Plot the distribution of sentiment values\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(x='Sentiment', data=train_data, order=[0, 1, 2, 3, 4])\n",
        "plt.title('Distribution of Sentiment Values')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(ticks=[0, 1, 2, 3, 4], labels=['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5ksnP-I2Fitd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize top 10 Countries that had the highest tweets using countplot (Tweet count vs Location)\n"
      ],
      "metadata": {
        "id": "_zc6AUq9Hry8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all location strings to lower case for consistency\n",
        "train_data['Location'] = train_data['Location'].str.lower()\n",
        "test_data['Location'] = test_data['Location'].str.lower()\n",
        "\n",
        "# Define a function to clean and map locations to countries\n",
        "def clean_location(location):\n",
        "    if re.search(r'\\b(new york|ny|nyc)\\b', location):\n",
        "        return 'united states'\n",
        "    elif re.search(r'\\b(london|england|uk|united kingdom|britain)\\b', location):\n",
        "        return 'united kingdom'\n",
        "    elif re.search(r'\\b(california|ca|los angeles|la|san francisco|sf)\\b', location):\n",
        "        return 'united states'\n",
        "    elif re.search(r'\\b(washington|dc)\\b', location):\n",
        "        return 'united states'\n",
        "    elif re.search(r'\\b(usa|us|u.s.|united states)\\b', location):\n",
        "        return 'united states'\n",
        "    elif re.search(r'\\bindia\\b', location):\n",
        "        return 'india'\n",
        "    elif re.search(r'\\b(australia|sydney|melbourne)\\b', location):\n",
        "        return 'australia'\n",
        "    elif re.search(r'\\b(canada|toronto|vancouver)\\b', location):\n",
        "        return 'canada'\n",
        "    elif re.search(r'\\b(germany|berlin)\\b', location):\n",
        "        return 'germany'\n",
        "    elif re.search(r'\\b(france|paris)\\b', location):\n",
        "        return 'france'\n",
        "    elif re.search(r'\\b(spain|madrid|barcelona)\\b', location):\n",
        "        return 'spain'\n",
        "    elif re.search(r'\\b(italy|rome|milan)\\b', location):\n",
        "        return 'italy'\n",
        "    elif re.search(r'\\b(brazil|rio|são paulo)\\b', location):\n",
        "        return 'brazil'\n",
        "    elif re.search(r'\\b(china|beijing|shanghai)\\b', location):\n",
        "        return 'china'\n",
        "    elif re.search(r'\\b(japan|tokyo)\\b', location):\n",
        "        return 'japan'\n",
        "    elif re.search(r'\\b(mexico|mexico city)\\b', location):\n",
        "        return 'mexico'\n",
        "    elif re.search(r'\\b(atlanta)\\b', location):\n",
        "        return 'united states'\n",
        "    elif re.search(r'\\b(boston)\\b', location):\n",
        "        return 'united states'\n",
        "    # Add more patterns as needed\n",
        "    else:\n",
        "        return location\n",
        "\n",
        "# Apply the clean_location function\n",
        "train_data['Location'] = train_data['Location'].apply(clean_location)\n",
        "test_data['Location'] = test_data['Location'].apply(clean_location)\n",
        "\n",
        "# Verify the changes\n",
        "print(\"Unique locations in train data:\")\n",
        "print(train_data['Location'].unique())\n",
        "\n",
        "print(\"\\nUnique locations in test data:\")\n",
        "print(test_data['Location'].unique())\n",
        "\n",
        "# Count the number of tweets per location again after cleaning\n",
        "top_10_locations = train_data['Location'].value_counts().head(10)\n",
        "\n",
        "# Plot the top 10 locations\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(y=train_data[train_data['Location'].isin(top_10_locations.index)]['Location'], order=top_10_locations.index)\n",
        "plt.title('Top 10 Locations with Highest Tweet Counts')\n",
        "plt.xlabel('Tweet Count')\n",
        "plt.ylabel('Location')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7uI9EsPBk17x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Plotting Pie Chart for the Sentiments in percentage\n"
      ],
      "metadata": {
        "id": "GUIM_P-VHuzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of each sentiment\n",
        "sentiment_percentage = train_data['Sentiment'].value_counts(normalize=True) * 100\n",
        "\n",
        "# Plot the pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(sentiment_percentage, labels=sentiment_percentage.index, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Sentiment Distribution in Percentage')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s8oRZOYDHAVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* WordCloud for the Tweets/Text\n",
        "\n",
        "    * Visualize the most commonly used words in each sentiment using wordcloud\n",
        "    * Refer to the following [link](https://medium.com/analytics-vidhya/word-cloud-a-text-visualization-tool-fb7348fbf502) for Word Cloud: A Text Visualization tool\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cSvzz5z6H8kM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot word cloud\n",
        "def plot_wordcloud(text, title):\n",
        "    wordcloud = WordCloud(width=800, height=400, random_state=21, max_font_size=110, background_color='white').generate(text)\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Plot word cloud for each sentiment\n",
        "for sentiment in train_data['Sentiment'].unique():\n",
        "    text = \" \".join(review for review in train_data[train_data['Sentiment'] == sentiment]['OriginalTweet'])\n",
        "    plot_wordcloud(text, f'Word Cloud for Sentiment {sentiment}')"
      ],
      "metadata": {
        "id": "vq7__byEHabv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tharun:- Thanks for the remark, will do that"
      ],
      "metadata": {
        "id": "n_C7qwbUDTTO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aqdg1DEaDKrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final model executed here\n"
      ],
      "metadata": {
        "id": "lT0NDfgfLm9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oLyIb19KcdL"
      },
      "source": [
        "##   **Stage 2**: Data Pre-Processing  (2 Points)\n",
        "\n",
        "####  Clean and Transform the data into a specified format\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "# Preprocess the text data\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'[^A-Za-z\\s]+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "train_data['OriginalTweet'] = train_data['OriginalTweet'].apply(preprocess_text)\n",
        "test_data['OriginalTweet'] = test_data['OriginalTweet'].apply(preprocess_text)\n",
        "\n",
        "# Encode the labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_data['Sentiment'] = label_encoder.fit_transform(train_data['Sentiment'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_data['OriginalTweet'], train_data['Sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "id": "sL_u6XluiQat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot word cloud for each sentiment\n",
        "for sentiment in train_data['Sentiment'].unique():\n",
        "    text = \" \".join(review for review in train_data[train_data['Sentiment'] == sentiment]['OriginalTweet'])\n",
        "    plot_wordcloud(text, f'Word Cloud for Sentiment {sentiment}'"
      ],
      "metadata": {
        "id": "g0MGW5sYlEsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EsaLlx2i3dSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZg7yL2TtTM"
      },
      "source": [
        "##   **Stage 3**: Build the Word Embeddings using pretrained Word2vec/Glove (Text Representation) (1 Point)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "_9rTpuiwSy0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = TweetDataset(X_train.tolist(), y_train.tolist(), tokenizer, max_len=128)\n",
        "val_dataset = TweetDataset(X_val.tolist(), y_val.tolist(), tokenizer, max_len=128)\n",
        "test_dataset = TweetDataset(test_data['OriginalTweet'].tolist(), [0] * len(test_data), tokenizer, max_len=128)\n"
      ],
      "metadata": {
        "id": "lxOwTID6kWiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fSytdzWGiRvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6jfm3YFUL7i"
      },
      "source": [
        "##   **Stage 4**: Build and Train the Deep Recurrent Model using Pytorch/Keras (4 Points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "Dz1-Bs4pUdsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] --upgrade\n",
        "!pip install accelerate --upgrade"
      ],
      "metadata": {
        "id": "r2YgFwx5HOOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2,  # Reduced epochs\n",
        "    per_device_train_batch_size=8,  # Smaller batch size\n",
        "    per_device_eval_batch_size=8,  # Smaller batch size\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()}\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "0DmRaeVXkn51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-O0Jx99UhmI"
      },
      "source": [
        "##   **Stage 5**: Evaluate the Model and get model predictions on the test dataset (2 Points)\n",
        "\n",
        "* Upload the model predictions to kaggle by mapping the sentiment column vlalues from numericals the categorical\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "PSaAlhGGUitF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "trainer.evaluate()\n",
        "\n",
        "# Make predictions\n",
        "predictions = trainer.predict(test_dataset)\n",
        "predicted_classes = predictions.predictions.argmax(axis=-1)\n",
        "\n",
        "# Map predicted classes back to sentiment labels\n",
        "reverse_sentiment_mapping = {v: k for k, v in sentiment_mapping.items()}\n",
        "test_data['Sentiment_Pred'] = predicted_classes\n",
        "test_data['Sentiment_Pred'] = test_data['Sentiment_Pred'].map(reverse_sentiment_mapping)\n",
        "\n",
        "# Prepare the submission file\n",
        "submission = test_data[['UserName', 'Sentiment_Pred']]\n",
        "submission.columns = ['Test_Id', 'Sentiment']\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Submission file saved as 'submission.csv'\")\n"
      ],
      "metadata": {
        "id": "usw5OoCikszC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKnc1WZk9cIk"
      },
      "source": [
        "### Instructions for preparing Kaggle competition predictions\n",
        "\n",
        "\n",
        "* Get the predictions using trained model and prepare a csv file\n",
        "    * DeepNet model gives output for each class, consider the maximum value among all classes as prediction using `np.argmax`.\n",
        "\n",
        "* Predictions (csv) file should contain 2 columns as Sample_Submission.csv\n",
        "  - First column is the Test_Id which is considered as index\n",
        "  - Second column is prediction in decoded form (for eg. Positive, Negative etc...)."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q85uowd_HkPB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}